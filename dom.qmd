---
title: "dom"
format: html
---

# Eng Gut clean
```{python}

import re

import spacy

# Load English spaCy model
nlp_en = spacy.load("en_core_web_sm")


#PART 0 - SETTING UP DOCUMENT
#with open("data/alice_english.txt", "r", encoding="utf-8") as f:
    #text = f.read()
    
start = "data/alice_english.txt"
end = "data/alice_english_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***"

# --- Find where the real book content begins and ends ---
#strtid = text.find(strtmkr)
#endid = text.find(endmkr)

#if strtid != -1:
    # Move to end of start marker line
    #strtid = text.find("\n", strtid) + 1  

#if endid != -1:
    # End right before the end marker
    #text_clean = text[strtid:endid].strip()
#else:
    #text_clean = text[strtid:].strip()
    
    
strtid = text.find(strtmkr)
endid = text.find(endmkr)

# If the start marker is found, move to end of its line
if strtid != -1:
    strtid = text.find("\n", strtid) + 1
else:
    strtid = 0  # start from beginning if not found

# If the end marker is not found, just go to the end of the text
if endid == -1:
    endid = len(text)

text_clean = text[strtid:endid].strip()

    
    
    
    
    
    

# --- Save the cleaned version ---


with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

with open("data/alice_english_clean.txt", "r", encoding="utf-8") as f:
    text_clean = f.read()

#alice_eng_words = re.split(r"[\s\W]+", text_clean)
#alice_eng_words = [w for w in alice_eng_words if w]
#alice_eng_clean = [w.lower() for w in 
#alice_eng_words]

alice_eng_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]



print(alice_eng_clean[:100])


alice_eng_word_count = len(alice_eng_clean)
print(alice_eng_word_count)



# with open(end, "w", encoding="utf-8") as f:
#     f.write(text_clean)
# 
# print("Cleaned file saved as:", output_file)
# 
# print("----")



```

# Fin Gut clean

```{python}


# Tokenize and lowercase words
alice_fi_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_fi_clean[:100])

# Count total words
alice_fi_word_count = len(alice_fi_clean)
print(alice_fi_word_count)



#PART 0 - SETTING UP DOCUMENT
with open("data/alice_finnish.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_finnish.txt"
end = "data/alice_finnish_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK LIISAN SEIKKAILUT IHMEMAASSA ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK LIISAN SEIKKAILUT IHMEMAASSA ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")


import re

# Tokenize and lowercase words
alice_fi_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_fi_clean[:100])

# Count total words
alice_fi_word_count = len(alice_fi_clean)
print(alice_fi_word_count)

```


# Fre Gut clean
```{python}
#PART 0 - SETTING UP DOCUMENT
with open("data/alice_french.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_french.txt"
end = "data/alice_french_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK AVENTURES D'ALICE AU PAYS DES MERVEILLES ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK AVENTURES D'ALICE AU PAYS DES MERVEILLES ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

import re

# Tokenize and lowercase words
alice_fr_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_fr_clean[:100])

# Count total words
alice_fr_word_count = len(alice_fr_clean)
print(alice_fr_word_count)

```


# Ita Gut clean

```{python}

#PART 0 - SETTING UP DOCUMENT
with open("data/alice_italian.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_italian.txt"
end = "data/alice_italian_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK LE AVVENTURE D'ALICE NEL PAESE DELLE MERAVIGLIE ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK LE AVVENTURE D'ALICE NEL PAESE DELLE MERAVIGLIE ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

import re

# Tokenize and lowercase words
alice_it_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_it_clean[:100])

# Count total words
alice_it_word_count = len(alice_it_clean)
print(alice_it_word_count)



```

# Spa Gut clean


```{python}
import re

# Load original file
with open("data/alice_spanish.txt", "r", encoding="utf-8") as f:
    text = f.read()


text_clean = text

# Save cleaned text
with open("data/alice_spanish_clean.txt", "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as: data/alice_spanish_clean.txt")

# Tokenize everything into lowercase words
words = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print diagnostics
print("First 100 words:", words[:100])
print("Total word count:", len(words))


```




# splitting



```{python}
import re

def load_and_tokenize(path):
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()

    # split by whitespace + punctuation
    words = re.split(r"[\s\W]+", text)
    words = [w.lower() for w in words if w]  # remove empty strings + lowercase
    return words

# Load all five languages
alice_english  = load_and_tokenize("data/alice_english_clean.txt")
alice_italian  = load_and_tokenize("data/alice_italian_clean.txt")
alice_french   = load_and_tokenize("data/alice_french_clean.txt")
alice_spanish  = load_and_tokenize("data/alice_spanish_clean.txt")
alice_finnish  = load_and_tokenize("data/alice_finnish_clean.txt")

# output
print("First 100 English words:", alice_english[:100])
print("English word count:", len(alice_english))

print("Italian word count:", len(alice_italian))
print("French word count:", len(alice_french))
print("Spanish word count:", len(alice_spanish))
print("Finnish word count:", len(alice_finnish))

```





```{python}
import spacy
import pandas as pd
from pathlib import Path

# 1. LOAD CLEANED TEXT FILES


files = {
    "English": "data/alice_english_clean.txt",
    "Italian": "data/alice_italian_clean.txt",
    "Spanish": "data/alice_spanish_clean.txt",
    "French":  "data/alice_french_clean.txt",
    "Finnish": "data/alice_finnish_clean.txt"
}

texts = {}
for lang, path in files.items():
    with open(path, "r", encoding="utf-8") as f:
        texts[lang] = f.read()

# 2. LOAD SPACY MODELS FOR EACH LANGUAGE


nlp_models = {
    "English": spacy.load("en_core_web_sm"),
    "Italian": spacy.load("it_core_news_sm"),
    "Spanish": spacy.load("es_core_news_sm"),
    "French":  spacy.load("fr_core_news_sm"),
    # Finnish does not have an official model; use 'fi_core_news_sm' if installed
    "Finnish": spacy.load("fi_core_news_sm")
}


# 3. PROCESS TEXTS + COMPUTE STATS


def compute_stats(doc):
    # count alphabetic tokens only
    tokens = [t for t in doc if t.is_alpha]

    num_tokens = len(tokens)
    unique_lemmas = len(set(t.lemma_.lower() for t in tokens))
    ttr = unique_lemmas / num_tokens if num_tokens > 0 else 0
    num_sentences = len(list(doc.sents))

    return num_tokens, unique_lemmas, ttr, num_sentences

results = {
    "Language": [],
    "Total Tokens": [],
    "Unique Lemmas": [],
    "Lexical Diversity (TTR)": [],
    "Sentences": []
}

for lang in texts:
    print(f"Processing {lang}â€¦")
    nlp = nlp_models[lang]
    doc = nlp(texts[lang])

    num_tokens, unique_lemmas, ttr, num_sentences = compute_stats(doc)

    results["Language"].append(lang)
    results["Total Tokens"].append(num_tokens)
    results["Unique Lemmas"].append(unique_lemmas)
    results["Lexical Diversity (TTR)"].append(round(ttr, 4))
    results["Sentences"].append(num_sentences)


# 4. CREATE TABLE

df = pd.DataFrame(results)
print("\n===== CORPUS COMPARISON TABLE =====\n")
print(df)


```
