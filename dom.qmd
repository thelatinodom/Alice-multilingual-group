---
title: "dom"
format: html
---


```{python}

#PART 0 - SETTING UP DOCUMENT
with open("data/alice_english.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_english.txt"
end = "data/alice_english_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---


with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

with open("data/alice_english_clean.txt", "r", encoding="utf-8") as f:
    text_clean = f.read()

alice_eng_words = re.split(r"[\s\W]+", text_clean)
alice_eng_words = [w for w in alice_eng_words if w]
alice_eng_clean = [w.lower() for w in alice_eng_words]
print(alice_eng_clean[:100])


alice_eng_word_count = 0 
for word in alice_eng_clean_clean:
  alice_eng_word_count = alice_eng_word_count + 1
print(alice_eng_word_count) 


# with open(end, "w", encoding="utf-8") as f:
#     f.write(text_clean)
# 
# print("Cleaned file saved as:", output_file)
# 
# print("----")



```

# Fin Gut clean

```{python}
#PART 0 - SETTING UP DOCUMENT
with open("data/alice_finnish.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_finnish.txt"
end = "data/alice_finnish_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK LIISAN SEIKKAILUT IHMEMAASSA ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK LIISAN SEIKKAILUT IHMEMAASSA ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")
```

# Fre Gut clean

```{python}
#PART 0 - SETTING UP DOCUMENT
with open("data/alice_french.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_french.txt"
end = "data/alice_french_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK AVENTURES D'ALICE AU PAYS DES MERVEILLES ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK AVENTURES D'ALICE AU PAYS DES MERVEILLES ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")
```

# Ita Gut clean

```{python}

#PART 0 - SETTING UP DOCUMENT
with open("data/alice_italian.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_italian.txt"
end = "data/alice_italian_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK LE AVVENTURE D'ALICE NEL PAESE DELLE MERAVIGLIE ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK LE AVVENTURE D'ALICE NEL PAESE DELLE MERAVIGLIE ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")
```

# Spa Gut clean

```{python}
#PART 0 - SETTING UP DOCUMENT
with open("data/alice_spanish.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_spanish.txt"
end = "data/alice_spanish_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** LEWIS CARROLL  (1832-1898) ***"
endmkr = "*** FIN ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")
```

```{python}
<<<<<<< HEAD
import re
with open("data/alice_english.txt", "r", encoding="utf-8") as f:
    idylls = f.read()

alice_eng_words = re.split(r"[\s\W]+", idylls)
alice_eng_words = [w for w in idylls_words if w]
alice_eng_clean = [w.lower() for w in idylls_words]
print(alice_clean[:100])

idylls_word_count = 0 
for word in idylls_clean:
  idylls_word_count = idylls_word_count + 1
print(idylls_word_count) 
=======
with open("data/alice_english_clean.txt")
with open("data/alice_italian_clean.txt")
with open("data/alice_french_clean.txt")
with open("data/alice_spanish_clean.txt")
with open("data/alice_finnish_clean.txt")




```

```{python}
import spacy
import pandas as pd
from pathlib import Path

# 1. LOAD CLEANED TEXT FILES


files = {
    "English": "data/alice_english_clean.txt",
    "Italian": "data/alice_italian_clean.txt",
    "Spanish": "data/alice_spanish_clean.txt",
    "French":  "data/alice_french_clean.txt",
    "Finnish": "data/alice_finnish_clean.txt"
}

texts = {}
for lang, path in files.items():
    with open(path, "r", encoding="utf-8") as f:
        texts[lang] = f.read()

# 2. LOAD SPACY MODELS FOR EACH LANGUAGE


nlp_models = {
    "English": spacy.load("en_core_web_sm"),
    "Italian": spacy.load("it_core_news_sm"),
    "Spanish": spacy.load("es_core_news_sm"),
    "French":  spacy.load("fr_core_news_sm"),
    # Finnish does not have an official model; use 'fi_core_news_sm' if installed
    "Finnish": spacy.load("fi_core_news_sm")
}


# 3. PROCESS TEXTS + COMPUTE STATS


def compute_stats(doc):
    # count alphabetic tokens only
    tokens = [t for t in doc if t.is_alpha]

    num_tokens = len(tokens)
    unique_lemmas = len(set(t.lemma_.lower() for t in tokens))
    ttr = unique_lemmas / num_tokens if num_tokens > 0 else 0
    num_sentences = len(list(doc.sents))

    return num_tokens, unique_lemmas, ttr, num_sentences

results = {
    "Language": [],
    "Total Tokens": [],
    "Unique Lemmas": [],
    "Lexical Diversity (TTR)": [],
    "Sentences": []
}

for lang in texts:
    print(f"Processing {lang}â€¦")
    nlp = nlp_models[lang]
    doc = nlp(texts[lang])

    num_tokens, unique_lemmas, ttr, num_sentences = compute_stats(doc)

    results["Language"].append(lang)
    results["Total Tokens"].append(num_tokens)
    results["Unique Lemmas"].append(unique_lemmas)
    results["Lexical Diversity (TTR)"].append(round(ttr, 4))
    results["Sentences"].append(num_sentences)


# 4. CREATE TABLE

df = pd.DataFrame(results)
print("\n===== CORPUS COMPARISON TABLE =====\n")
print(df)

>>>>>>> 8e83384 (Doing these tasks: Manually remove headers and footers. Then, load and clean all language files and tokenize using the appropriate spaCy pipeline for each language.)
```
