---
title: "HW7_alice"
date: "2025-11-19"
format: html
---

# Section 1: Dom

## Eng Gut Clean

```{python}
import re
import spacy

#PART 0 - SETTING UP DOCUMENT
    
start = "data/alice_english.txt"
end = "data/alice_english_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***"
    
    
strtid = text.find(strtmkr)
endid = text.find(endmkr)

# If the start marker is found, move to end of its line
if strtid != -1:
    strtid = text.find("\n", strtid) + 1
else:
    strtid = 0  # start from beginning if not found

# If the end marker is not found, just go to the end of the text
if endid == -1:
    endid = len(text)

text_clean = text[strtid:endid].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

with open("data/alice_english_clean.txt", "r", encoding="utf-8") as f:
    text_clean = f.read()

alice_eng_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

print(alice_eng_clean[:100])

alice_eng_word_count = len(alice_eng_clean)
print(alice_eng_word_count)

```

## Fin Gut clean

```{python}

#PART 0 - SETTING UP DOCUMENT
with open("data/alice_finnish.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_finnish.txt"
end = "data/alice_finnish_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "*** START OF THE PROJECT GUTENBERG EBOOK LIISAN SEIKKAILUT IHMEMAASSA ***"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK LIISAN SEIKKAILUT IHMEMAASSA ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")


import re

# Tokenize and lowercase words
alice_fi_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_fi_clean[:100])

# Count total words
alice_fi_word_count = len(alice_fi_clean)
print(alice_fi_word_count)

```

## Fre Gut clean

```{python}
#PART 0 - SETTING UP DOCUMENT
with open("data/alice_french.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_french.txt"
end = "data/alice_french_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = " La liste des modifications se trouve à la fin du texte."
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK AVENTURES D'ALICE AU PAYS DES MERVEILLES ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

import re

# Tokenize and lowercase words
alice_fr_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_fr_clean[:100])

# Count total words
alice_fr_word_count = len(alice_fr_clean)
print(alice_fr_word_count)

```

## Ita Gut clean

```{python}

#PART 0 - SETTING UP DOCUMENT
with open("data/alice_italian.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
start = "data/alice_italian.txt"
end = "data/alice_italian_clean.txt"

with open(start, "r", encoding="utf-8") as f:
    text = f.read()

# --- Identify Gutenberg markers ---
strtmkr = "http://www.archive.org/details/leavventuredalic00carr"
endmkr = "*** END OF THE PROJECT GUTENBERG EBOOK LE AVVENTURE D'ALICE NEL PAESE DELLE MERAVIGLIE ***"

# --- Find where the real book content begins and ends ---
strtid = text.find(strtmkr)
endid = text.find(endmkr)

if strtid != -1:
    # Move to end of start marker line
    strtid = text.find("\n", strtid) + 1  

if endid != -1:
    # End right before the end marker
    text_clean = text[strtid:endid].strip()
else:
    text_clean = text[strtid:].strip()

# --- Save the cleaned version ---

with open(end, "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as:", end)
print("----")

import re

# Tokenize and lowercase words
alice_it_clean = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print first 100 words
print(alice_it_clean[:100])

# Count total words
alice_it_word_count = len(alice_it_clean)
print(alice_it_word_count)

```

## Spa Gut clean

```{python}
import re

# Load original file
with open("data/alice_spanish.txt", "r", encoding="utf-8") as f:
    text = f.read()


text_clean = text

# Save cleaned text
with open("data/alice_spanish_clean.txt", "w", encoding="utf-8") as f:
    f.write(text_clean)

print("Cleaned file saved as: data/alice_spanish_clean.txt")

# Tokenize everything into lowercase words
words = [w.lower() for w in re.split(r"[\s\W]+", text_clean) if w]

# Print diagnostics
print("First 100 words:", words[:100])
print("Total word count:", len(words))

```

## splitting

```{python}
import re

def load_and_tokenize(path):
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()

    # split by whitespace + punctuation
    words = re.split(r"[\s\W]+", text)
    words = [w.lower() for w in words if w]  # remove empty strings + lowercase
    return words

# Load all five languages
alice_english  = load_and_tokenize("data/alice_english_clean.txt")
alice_italian  = load_and_tokenize("data/alice_italian_clean.txt")
alice_french   = load_and_tokenize("data/alice_french_clean.txt")
alice_spanish  = load_and_tokenize("data/alice_spanish_clean.txt")
alice_finnish  = load_and_tokenize("data/alice_finnish_clean.txt")

# output
print("First 100 English words:", alice_english[:100])
print("English word count:", len(alice_english))

print("Italian word count:", len(alice_italian))
print("French word count:", len(alice_french))
print("Spanish word count:", len(alice_spanish))
print("Finnish word count:", len(alice_finnish))

```

## Spacy Corpus Table

```{python}
import spacy
import pandas as pd
from pathlib import Path

# 1. LOAD CLEANED TEXT FILES


files = {
    "English": "data/alice_english_clean.txt",
    "Italian": "data/alice_italian_clean.txt",
    "Spanish": "data/alice_spanish_clean.txt",
    "French":  "data/alice_french_clean.txt",
    "Finnish": "data/alice_finnish_clean.txt"
}

texts = {}
for lang, path in files.items():
    with open(path, "r", encoding="utf-8") as f:
        texts[lang] = f.read()

# 2. LOAD SPACY MODELS FOR EACH LANGUAGE


nlp_models = {
    "English": spacy.load("en_core_web_sm"),
    "Italian": spacy.load("it_core_news_sm"),
    "Spanish": spacy.load("es_core_news_sm"),
    "French":  spacy.load("fr_core_news_sm"),
    # Finnish does not have an official model; use 'fi_core_news_sm' if installed
    "Finnish": spacy.load("fi_core_news_sm")
}


# 3. PROCESS TEXTS + COMPUTE STATS


def compute_stats(doc):
    # count alphabetic tokens only
    tokens = [t for t in doc if t.is_alpha]

    num_tokens = len(tokens)
    unique_lemmas = len(set(t.lemma_.lower() for t in tokens))
    ttr = unique_lemmas / num_tokens if num_tokens > 0 else 0
    num_sentences = len(list(doc.sents))

    return num_tokens, unique_lemmas, ttr, num_sentences

results = {
    "Language": [],
    "Total Tokens": [],
    "Unique Lemmas": [],
    "Lexical Diversity (TTR)": [],
    "Sentences": []
}

for lang in texts:
    print(f"Processing {lang}…")
    nlp = nlp_models[lang]
    doc = nlp(texts[lang])

    num_tokens, unique_lemmas, ttr, num_sentences = compute_stats(doc)

    results["Language"].append(lang)
    results["Total Tokens"].append(num_tokens)
    results["Unique Lemmas"].append(unique_lemmas)
    results["Lexical Diversity (TTR)"].append(round(ttr, 4))
    results["Sentences"].append(num_sentences)


# 4. CREATE TABLE

df = pd.DataFrame(results)
print("\n===== CORPUS COMPARISON TABLE =====\n")
print(df)
# saving the dataframe
df.to_csv('figures/Section1_DataFrame.csv')


```

# Section 2: ERIN

```{python}
import spacy
from nltk.corpus import wordnet as wn
import pandas as pd
import matplotlib.pyplot as plt
import nltk
from collections import Counter
import itertools
from wordcloud import WordCloud
import numpy as np

#ENG
with open("data/alice_english_clean.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
nlp = spacy.load("en_core_web_sm")    
doc = nlp(text)    

content_pos = {"NOUN", "VERB", "ADJ", "ADV"}

eng_lemma_list = [
    (token.lemma_.lower(), token.pos_)
    for token in doc
    if token.pos_ in content_pos and token.is_alpha
]

eng_lemmas = [lemma for lemma, pos in eng_lemma_list]
eng_lemma_freq = Counter(eng_lemmas)

print("The top 20 Lemmas for English are",eng_lemma_freq.most_common(20))

eng_df = pd.DataFrame(
    eng_lemma_freq.items(),
    columns=["lemma", "frequency"]
)

eng_df = eng_df.sort_values(by="frequency", ascending=False)

print(eng_df.head(20))

eng_top20 = eng_df.head(20)

eng_freq = dict(zip(eng_top20["lemma"], eng_top20["frequency"]))

eng_wc = WordCloud(
    width=800,
    height=400,
    background_color="white"
).generate_from_frequencies(eng_freq)

plt.figure()
plt.imshow(eng_wc, interpolation="bilinear")
plt.axis("off")
plt.title("Top 20 Lemmas – English Alice in Wonderland")
plt.tight_layout()
plt.savefig('figures/Top_Lemmas_English.png', dpi=740, bbox_inches='tight')
plt.show()

print("--")


#ITALIAN
with open("data/alice_italian_clean.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
nlp = spacy.load("it_core_news_sm")    
doc = nlp(text)    

content_pos = {"NOUN", "VERB", "ADJ", "ADV"}

it_lemma_list = [
    (token.lemma_.lower(), token.pos_)
    for token in doc
    if token.pos_ in content_pos and token.is_alpha
]

it_lemmas = [lemma for lemma, pos in it_lemma_list]
it_lemma_freq = Counter(it_lemmas)

print("The top 20 Lemmas for Italian are", it_lemma_freq.most_common(20))

it_df = pd.DataFrame(
    it_lemma_freq.items(),
    columns=["lemma", "frequency"]
)

it_df = it_df.sort_values(by="frequency", ascending=False)

print(it_df.head(20))

it_top20 = it_df.head(20)

it_freq = dict(zip(it_top20["lemma"], it_top20["frequency"]))

it_wc = WordCloud(
    width=800,
    height=400,
    background_color="white"
).generate_from_frequencies(it_freq)

plt.figure()
plt.imshow(it_wc, interpolation="bilinear")
plt.axis("off")
plt.title("Top 20 Lemmas – Italian Alice in Wonderland")
plt.tight_layout()
plt.savefig('figures/Top_Lemmas_Italian.png', dpi=740, bbox_inches='tight')
plt.show()

print("--")

#SPANISH
with open("data/alice_spanish_clean.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
nlp = spacy.load("es_core_news_sm")    
doc = nlp(text)    

content_pos = {"NOUN", "VERB", "ADJ", "ADV"}

spa_lemma_list = [
    (token.lemma_.lower(), token.pos_)
    for token in doc
    if token.pos_ in content_pos and token.is_alpha
]

spa_lemmas = [lemma for lemma, pos in spa_lemma_list]
spa_lemma_freq = Counter(spa_lemmas)

print("The top 20 Lemmas for Spanish are", spa_lemma_freq.most_common(20))

spa_df = pd.DataFrame(
    spa_lemma_freq.items(),
    columns=["lemma", "frequency"]
)

spa_df = spa_df.sort_values(by="frequency", ascending=False)

print(spa_df.head(20))

spa_top20 = spa_df.head(20)

spa_freq = dict(zip(spa_top20["lemma"], spa_top20["frequency"]))

spa_wc = WordCloud(
    width=800,
    height=400,
    background_color="white"
).generate_from_frequencies(spa_freq)

plt.figure()
plt.imshow(spa_wc, interpolation="bilinear")
plt.axis("off")
plt.title("Top 20 Lemmas – Spanish Alice in Wonderland")
plt.tight_layout()
plt.savefig('figures/Top_Lemmas_Spanish.png', dpi=740, bbox_inches='tight')
plt.show()

print("--")

#FRENCH
with open("data/alice_french_clean.txt", "r", encoding="utf-8") as f:
    text = f.read()

nlp = spacy.load("fr_core_news_sm")    
doc = nlp(text)    

content_pos = {"NOUN", "VERB", "ADJ", "ADV"}

fr_lemma_list = [
    (token.lemma_.lower(), token.pos_)
    for token in doc
    if token.pos_ in content_pos and token.is_alpha
]

fr_lemmas = [lemma for lemma, pos in fr_lemma_list]
fr_lemma_freq = Counter(fr_lemmas)

print("The top 20 Lemmas for French are", fr_lemma_freq.most_common(20))

fr_df = pd.DataFrame(
    fr_lemma_freq.items(),
    columns=["lemma", "frequency"]
)

fr_df = fr_df.sort_values(by="frequency", ascending=False)

print(fr_df.head(20))

fr_top20 = fr_df.head(20)

fr_freq = dict(zip(fr_top20["lemma"], fr_top20["frequency"]))

fr_wc = WordCloud(
    width=800,
    height=400,
    background_color="white"
).generate_from_frequencies(fr_freq)

plt.figure()
plt.imshow(fr_wc, interpolation="bilinear")
plt.axis("off")
plt.title("Top 20 Lemmas – French Alice in Wonderland")
plt.tight_layout()
plt.savefig('figures/Top_Lemmas_French.png', dpi=740, bbox_inches='tight')
plt.show()

print("--")

#FINNISH
with open("data/alice_finnish_clean.txt", "r", encoding="utf-8") as f:
    text = f.read()
    
nlp = spacy.load("fi_core_news_sm")    
doc = nlp(text)    

content_pos = {"NOUN", "VERB", "ADJ", "ADV"}

fin_lemma_list = [
    (token.lemma_.lower(), token.pos_)
    for token in doc
    if token.pos_ in content_pos and token.is_alpha
]

fin_lemmas = [lemma for lemma, pos in fin_lemma_list]
fin_lemma_freq = Counter(fin_lemmas)

print("The top 20 Lemmas for Finnish are", fin_lemma_freq.most_common(20))

fin_df = pd.DataFrame(
    fin_lemma_freq.items(),
    columns=["lemma", "frequency"]
)

fin_df = fin_df.sort_values(by="frequency", ascending=False)

print(fin_df.head(20))

fin_top20 = fin_df.head(20)

fin_freq = dict(zip(fin_top20["lemma"], fin_top20["frequency"]))

fin_wc = WordCloud(
    width=800,
    height=400,
    background_color="white"
).generate_from_frequencies(fin_freq)

plt.figure()
plt.imshow(fin_wc, interpolation="bilinear")
plt.axis("off")
plt.title("Top 20 Lemmas – Finnish Alice in Wonderland")
plt.tight_layout()
plt.savefig('figures/Top_Lemmas_Finnish.png', dpi=740, bbox_inches='tight')
plt.show()

print("----")

fig, axes = plt.subplots(1, 5, figsize=(40, 10))  

# English
axes[0].barh(eng_top20["lemma"][::-1], eng_top20["frequency"][::-1], color='blue')
axes[0].set_title("English")
axes[0].set_xlabel("Frequency")

# Italian
axes[1].barh(it_top20["lemma"][::-1], it_top20["frequency"][::-1], color='green')
axes[1].set_title("Italian")
axes[1].set_xlabel("Frequency")

# Spanish
axes[2].barh(spa_top20["lemma"][::-1], spa_top20["frequency"][::-1], color='red')
axes[2].set_title("Spanish")
axes[2].set_xlabel("Frequency")

# French
axes[3].barh(fr_top20["lemma"][::-1], fr_top20["frequency"][::-1], color='purple')
axes[3].set_title("French")
axes[3].set_xlabel("Frequency")

# Finnish
axes[4].barh(fin_top20["lemma"][::-1], fin_top20["frequency"][::-1], color='orange')
axes[4].set_title("Finnish")
axes[4].set_xlabel("Frequency")

fig.suptitle( "Top 20 Most Frequent Content Lemmas Across Languages – Alice in Wonderland", fontsize=18)

fig.subplots_adjust(wspace=.75, top=0.85)  

plt.savefig('figures/Content_Lemmas_Across_Languages.png', dpi=740, bbox_inches='tight')

plt.show()

```

The only lemmas, that seem to directly translate to each other, that are present in the top 20 across all language translations in the study were 'say', which was ranked 1st, for most used, and 'see'. 'know', 'do/make', 'have', 'time', 'head' appear across four of the five language translations.

Within the Romance Language translations, all three had seven of the same lemmas present in the top 20: 'no/not', 'say', 'make/do', 'have', 'see', 'more/most' and 'know'; with 'no/not' and 'more/most' lemmas only being found in the Romance Languages' top lemmas as a subcategory.

Finnish lemmas were the most unique, having 8 translations in the top 20 that were not present in the top 20 of other translations: 'quite/totally' \[aivan\], 'only/just' \[vain\], 'king' \[kuningas\], 'ask/inquire' \[kysyae\], 'hear' \[kuulla\], 'root/origin' \[juuri\], 'hatter' \[hatuntekijae\], 'how/why' \[kuinka\]. All other translations had 3-5 unique lemmas in their top 20.

Alice was counted as its own lemma only in Italian and French.

The English translation has the sharpest drop-off in lemma usage, going from 'say' at 532, to 'go' at 179. Finnish has the smallest going from 'say' at 293 to 'so/like-that' \[niin\] at 182.

# Section 3: Bellamy

```{python}
import spacy
import pandas as pd
from collections import Counter

#VERBS

nlp_models = {
    'English': 'en_core_web_sm',
    'French': 'fr_core_news_sm',
    'Spanish': 'es_core_news_sm',
    'Italian': 'it_core_news_sm',
    'Finnish': 'fi_core_news_sm'
}

files = {
    "English": "data/alice_english_clean.txt",
    "Italian": "data/alice_italian_clean.txt",
    "Spanish": "data/alice_spanish_clean.txt",
    "French":  "data/alice_french_clean.txt",
    "Finnish": "data/alice_finnish_clean.txt"
}

name_alts = {
    'English': ['Alice', 'alice'],
    'French': ['Alice', 'alice'],
    'Spanish': ['Alicia', 'alicia'],
    'Italian': ['Alice', 'alice'],
    'Finnish': ['Liisa', 'liisa']
}

def extract_alice_verbs(text, nlp, language):
    """
    Extract verbs where Alice is the subject or object.
    
    Arguments:
        text: The text to analyze
        nlp: The spaCy language model
        language: The language being analyzed (for Alice name variations)
    
    Returns:
        List of verb lemmas
    """
    doc = nlp(text)
    alice_verbs = []
    alice_names = name_alts[language]
    
    for token in doc:
        # Check if token is Alice
        if token.text in alice_names:
            # Check if Alice is the subject (nsubj) of a verb
            if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':
                alice_verbs.append(token.head.lemma_.lower())
            
            # Check if Alice is the object (dobj) of a verb
            elif token.dep_ == 'dobj' and token.head.pos_ == 'VERB':
                alice_verbs.append(token.head.lemma_.lower())
        
        # Also check if any token has Alice as its subject or object
        for child in token.children:
            if child.text in alice_names:
                if child.dep_ == 'nsubj' and token.pos_ == 'VERB':
                    alice_verbs.append(token.lemma_.lower())
                elif child.dep_ == 'dobj' and token.pos_ == 'VERB':
                    alice_verbs.append(token.lemma_.lower())
    
    return alice_verbs

def analyze_language(language, file_path, model_name):
    """
    Analyze a single language version of Alice in Wonderland.
    
    Arguments:
        language: Name of the language
        file_path: Path to the text file
        model_name: Name of the spaCy model to use
    
    Returns:
        Counter object with verb frequencies
    """
    nlp = spacy.load(model_name)

    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    verbs = extract_alice_verbs(text, nlp, language)
    
    return Counter(verbs)

def create_summary_table(all_results):
    """
    Create a summary table with top 5 verbs for each language.
    
    Arguments:
        all_results: Dictionary mapping language names to Counter objects
    
    Returns:
        pandas DataFrame with results
    """
    summary_data = []
    
    for language, verb_counts in all_results.items():
        top_5 = verb_counts.most_common(5)
        for rank, (verb, count) in enumerate(top_5, 1):
            summary_data.append({
                'Language': language.capitalize(),
                'Rank': rank,
                'Verb': verb,
                'Frequency': count
            })
    
    df = pd.DataFrame(summary_data)
    return df

def main():
    """Main function to process all languages."""
    all_results = {}
    
    # processes each language
    for language, model_name in nlp_models.items():
        try:
            file_path = files[language]
            verb_counts = analyze_language(language, file_path, model_name)
            all_results[language] = verb_counts
            
            print(f"\n{language.capitalize()} - Top 10 verbs:")
            for verb, count in verb_counts.most_common(10):
                print(f"  {verb}: {count}")
            print()
        
        #TO KEEP EVERYTHING FROM CRASHING       
        except Exception as e:
            print(f"Error processing {language}: {e}")
            continue
    
    # creates summary table
    summary_df = create_summary_table(all_results)
    print(summary_df.to_string(index=False))
    
    # save to CSV
    summary_df.to_csv('figures/alice_verbs_summary.csv', index=False)
    
    return all_results, summary_df

# run the analysis
if __name__ == "__main__":
    results, summary = main()
    
    
    
    
#ADJECTIVES

nlp_models = {
    'English': 'en_core_web_sm',
    'French': 'fr_core_news_sm',
    'Spanish': 'es_core_news_sm',
    'Italian': 'it_core_news_sm',
    'Finnish': 'fi_core_news_sm'
}

files = {
    "English": "data/alice_english_clean.txt",
    "Italian": "data/alice_italian_clean.txt",
    "Spanish": "data/alice_spanish_clean.txt",
    "French":  "data/alice_french_clean.txt",
    "Finnish": "data/alice_finnish_clean.txt"
}

name_alts = {
    'English': ['Alice', 'alice'],
    'French': ['Alice', 'alice'],
    'Spanish': ['Alicia', 'alicia'],
    'Italian': ['Alice', 'alice'],
    'Finnish': ['Liisa', 'liisa']
}

def extract_alice_adjectives(text, nlp, language):
    """
    Extract adjectives that modify Alice.
    
    Arguments:
        text: The text to analyze
        nlp: The spaCy language model
        language: The language being analyzed (for Alice name variations)
    
    Returns:
        List of adjective lemmas
    """
    doc = nlp(text)
    alice_adjectives = []
    alice_names = name_alts[language]
    
    for token in doc:
        # Check if token is Alice
        if token.text in alice_names:
            # Look for adjectives that modify Alice
            # Common patterns: amod (adjectival modifier), acomp (adjectival complement)
            for child in token.children:
                if child.pos_ == 'ADJ' and child.is_alpha:
                    alice_adjectives.append(child.lemma_.lower())
            
            # Also check if Alice is modifying something (less common but possible)
            if token.head.pos_ == 'ADJ' and token.head.is_alpha:
                # Check the dependency relation
                if token.dep_ in ['nsubj', 'attr', 'nmod']:
                    alice_adjectives.append(token.head.lemma_.lower())
        
        # Check if token is an adjective modifying Alice
        if token.pos_ == 'ADJ' and token.is_alpha:
            for child in token.children:
                if child.text in alice_names:
                    alice_adjectives.append(token.lemma_.lower())
    
    return alice_adjectives

def analyze_language(language, file_path, model_name):
    """
    Analyze a single language version of Alice in Wonderland.
    
    Arguments:
        language: Name of the language
        file_path: Path to the text file
        model_name: Name of the spaCy model to use
    
    Returns:
        Counter object with adjective frequencies
    """
    nlp = spacy.load(model_name)

    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    adjectives = extract_alice_adjectives(text, nlp, language)
    
    return Counter(adjectives)

def create_summary_table(all_results):
    """
    Create a summary table with top 5 adjectives for each language.
    
    Arguments:
        all_results: Dictionary mapping language names to Counter objects
    
    Returns:
        pandas DataFrame with results
    """
    summary_data = []
    
    for language, adj_counts in all_results.items():
        top_5 = adj_counts.most_common(5)
        for rank, (adjective, count) in enumerate(top_5, 1):
            summary_data.append({
                'Language': language.capitalize(),
                'Rank': rank,
                'Adjective': adjective,
                'Frequency': count
            })
    
    df = pd.DataFrame(summary_data)
    return df

def main():
    """Main function to process all languages."""
    all_results = {}
    
    # processes each language
    for language, model_name in nlp_models.items():
        try:
            file_path = files[language]
            adj_counts = analyze_language(language, file_path, model_name)
            all_results[language] = adj_counts
            
            print(f"\n{language.capitalize()} - Top 10 adj")
            for adjective, count in adj_counts.most_common(10):
                print(f"  {adjective}: {count}")
            print()
        
        #TO KEEP EVERYTHING FROM CRASHING       
        except Exception as e:
            print(f"Error processing {language}: {e}")
            continue
    
    # creates the summary table
    summary_df = create_summary_table(all_results)
    print(summary_df.to_string(index=False))
    
    # save to CSV
    summary_df.to_csv('figures/alice_adjectives_summary.csv', index=False)
    
    return all_results, summary_df

# run the analysis
if __name__ == "__main__":
    results, summary = main()
```

**English**: The top verbs feel very true to the story. Alice is always "saying", "thinking", "replying", "beginning", or "looking" or those actions are being directed at her. The top adjectives also make sense and reveal the confusing and often uncomfortable scenarios Alice finds herself in.

**French**: The top verbs in French are similar to those in English. "Dire" is equivalent to "to say", "trouver" is "to find" and may also connote having an opinion/experience of something and so can relate to "thinking", "commencer" is "to begin", "sentir" is "to feel", and "savoir" is "to know". The adjectives also match the English, with the top five translating to "alone", "noticed" or "remarkable" (and in the sense of the story could be "peculiar"), "little", "indignant", and "hauty/proud". It is interesting that the French version seems to drop Alice's hunger, or at least focuses less on it, and instead focuses on how precocious she is.

**Spanish**: The Spanish verbs also follow the pattern of "thinking", "saying", "asking", "looking", and "beginning". The adjectives additionally have high counts of "poor" and "little" but also include "scared", which may be a way of communicating Alice's discomfort with her situation. However, there's also a high count of "interested", which reflects Alice's engagement with all that is "new" around her.

**Italian**: The Italian verbs keep up with "saying", "responding/answering", "thinking", and "asking", and also includes "soggiunse" meaning "to add" (to a conversation). The Italian adjectives maintain the pattern of "poor" Alice, and continues Spanish's interest in all that is "new" and Alice being "amazed" at what she sees. It adds the new adjective of "all", which is interesting as it stands in opposition to English and French's emphasis on "alone", and might apply to the groups which Alice observes.

**Finnish**: Finnish rounds out the collection of verbs with "saying", "answering", "asking", and "exclaiming", but also adds "believing" to the mix, giving the first instance of Alice's deeper questioning of the world she finds herself in and the new social norms she has to naviagte. As for adjectives, "white" leads the list, followed by "ready", "scared", "cheerful", and once again "little". It seems that Finnish focuses more on Alice's mood than her physical state, but it is also the first language to have a repeat of English's "ready", perhaps leaning into Alice's self-questioning within the story.

# Section 4: Dylan

```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# ENGLISH

en_nlp = spacy.load("en_core_web_sm")
en_nlp.add_pipe("spacy_wordnet", after='tagger')

en_lemmas = ["", "say", "go", "think", "know", "see", "time", "head", "queen", "king", "caterpillar"]
en_text = " ".join(en_lemmas)
en_doc = en_nlp(en_text)

for token in en_doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()
    if wn_pos:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_})")
        for s in synsets[:3]:
            hypernym_names = [h.name() for h in s.hypernyms()]
            print(f"  - {s.definition()} ... Hypernyms: {hypernym_names}")

print("-----")

say = en_nlp("say")[0]._.wordnet.synsets()[0]
comparison1 = en_nlp("think")[0]._.wordnet.synsets()[0]

print("The Wu-Palmer similarity between 'say' and 'think' is:",  say.wup_similarity(comparison1))

king = en_nlp("king")[0]._.wordnet.synsets()[0]
comparison2 = en_nlp("caterpillar")[0]._.wordnet.synsets()[0]

print("The Wu-Palmer similarity between 'king' and 'caterpillar' is:",  king.wup_similarity(comparison2))

print("-----------------")

# ITALIAN

it_nlp = spacy.load("it_core_news_sm")
it_nlp.add_pipe("spacy_wordnet", after='tagger')

it_lemmas = ["", "dire", "andare", "pensare", "sapere", "vedere", "tempo", "testa", "regina", "re", "bruco"]
it_text = " ".join(it_lemmas)
it_doc = it_nlp(it_text)

for token in it_doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()
    if wn_pos:
        synsets = wn.synsets(lemma, pos=wn_pos, lang='ita')
        print(f"\n{token.text.upper()} ({token.pos_})")
        for s in synsets[:3]:
            hypernym_names = [h.name() for h in s.hypernyms()]
            print(f"  - {s.definition()} ... Hypernyms: {hypernym_names}")

print("-----")

dire_synsets = it_nlp("dire")[0]._.wordnet.synsets()
comparison1_synsets = it_nlp("pensare")[0]._.wordnet.synsets()

if dire_synsets and comparison1_synsets:
    dire = dire_synsets[0]
    comparison1 = comparison1_synsets[0]
    print("The Wu-Palmer similarity between 'dire' and 'pensare' is:", dire.wup_similarity(comparison1))
else:
    print("No WordNet synsets found for 'dire' or 'pensare'")


print("--")

re_synsets = it_nlp("re")[0]._.wordnet.synsets()
comparison2_synsets = it_nlp("bruco")[0]._.wordnet.synsets()

if re_synsets and comparison2_synsets:
    re = re_synsets[0]
    comparison2 = comparison2_synsets[0]
    print("The Wu-Palmer similarity between 're' and 'bruco' is:", re.wup_similarity(comparison2))
else:
    print("No WordNet synsets found for 're' and 'bruco'")    
    
print("-----------------")

# SPANISH

es_nlp = spacy.load("es_core_news_sm")
es_nlp.add_pipe("spacy_wordnet", after='morphologizer')

es_lemmas = ["", "decir", "ir", "pensar", "saber", "ver", "tiempo", "la cabeza", "la reina", "el rey", "la oruga"]
es_text = " ".join(es_lemmas)
es_doc = es_nlp(es_text)

for token in es_doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()
    if wn_pos:
        synsets = wn.synsets(lemma, pos=wn_pos, lang='spa')
        print(f"\n{token.text.upper()} ({token.pos_})")
        for s in synsets[:3]:
            hypernym_names = [h.name() for h in s.hypernyms()]
            print(f"  - {s.definition()} ... Hypernyms: {hypernym_names}")

print("-----")

decir_synsets = es_nlp("decir")[0]._.wordnet.synsets()
comparison1_synsets = es_nlp("pensar")[0]._.wordnet.synsets()

if decir_synsets and comparison1_synsets:
    decir = decir_synsets[0]
    comparison1 = comparison1_synsets[0]
    print("The Wu-Palmer similarity between 'decir' and 'pensar' is:",  decir.wup_similarity(comparison1))
else: 
  print("No WordNet synsets found for 'decir' and 'pensar'")  
  
print("--")

rey_synsets = es_nlp("rey")[0]._.wordnet.synsets()
comparison2_synsets = es_nlp("oruga")[0]._.wordnet.synsets()

if rey_synsets and comparison2_synsets:
    rey = rey_synsets[0]
    comparison2 = comparison2_synsets[0]
    print("The Wu-Palmer similarity between 'rey' and 'oruga' is:",  rey.wup_similarity(comparison2))
else: 
  print("No WordNet synsets found for 'rey' and 'oruga'")  

print("-----------------")

# FRENCH

fr_nlp = spacy.load("fr_core_news_sm")
fr_nlp.add_pipe("spacy_wordnet", after='morphologizer')

fr_lemmas = ["", "dire", "aller", "penser", "savoir", "voir", "temps", "tête", "reine", "roi", "chenille"]
fr_text = " ".join(fr_lemmas)
fr_doc = fr_nlp(fr_text)

for token in fr_doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()
    if wn_pos:
        synsets = wn.synsets(lemma, pos=wn_pos, lang='fra')
        print(f"\n{token.text.upper()} ({token.pos_})")
        for s in synsets[:3]:
            hypernym_names = [h.name() for h in s.hypernyms()]
            print(f"  - {s.definition()} ... Hypernyms: {hypernym_names}")

print("-----")

dire_synsets = fr_nlp("dire")[0]._.wordnet.synsets()
comparison1_synsets = fr_nlp("penser")[0]._.wordnet.synsets()

if dire_synsets and comparison1_synsets:
    dire = dire_synsets[0]
    comparison1 = comparison1_synsets[0]
    print("The Wu-Palmer similarity between 'dire' and 'penser' is:",  dire.wup_similarity(comparison1))
else: 
  print("No WordNet synsets found for 'decir' and 'pensar'")  
  
print("--")

roi_synsets = fr_nlp("roi")[0]._.wordnet.synsets()
comparison2_synsets = fr_nlp("chenille")[0]._.wordnet.synsets()

if dire_synsets and comparison2_synsets:
    roi = roi_synsets[0]
    comparison2 = comparison2_synsets[0]
    print("The Wu-Palmer similarity between 'roi' and 'chenille' is:",  roi.wup_similarity(comparison2))
else: 
  print("No WordNet synsets found for 'roi' and 'chenille'")  

print("-----------------")

# FINNISH

fi_nlp = spacy.load("fi_core_news_sm")
fi_nlp.add_pipe("spacy_wordnet", after='morphologizer')

fi_lemmas = ["", "sanoa", "mennä", "ajatella", "tietää", "nähdä", "aika", "pää", "kuningatar", "kuningas", "toukka"]
fi_text = " ".join(fi_lemmas)
fi_doc = fi_nlp(fi_text)

for token in fi_doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()
    if wn_pos:
        synsets = wn.synsets(lemma, pos=wn_pos, lang='fin')
        print(f"\n{token.text.upper()} ({token.pos_})")
        for s in synsets[:3]:
            hypernym_names = [h.name() for h in s.hypernyms()]
            print(f"  - {s.definition()} ... Hypernyms: {hypernym_names}")

print("-----")

sanoa = fi_nlp("sanoa")[0]._.wordnet.synsets()[0]
comparison1 = fi_nlp("ajatella")[0]._.wordnet.synsets()[0]

print("The Wu-Palmer similarity between 'sanoa' and 'ajatella' is:",  sanoa.wup_similarity(comparison1))

kuningas = fi_nlp("kuningas")[0]._.wordnet.synsets()[0]
comparison2 = fi_nlp("toukka")[0]._.wordnet.synsets()[0]

print("The Wu-Palmer similarity between 'kuningas' and 'toukka' is:",  kuningas.wup_similarity(comparison2))
```
